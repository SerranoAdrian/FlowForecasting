{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import random\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from itertools import chain\n",
    "\n",
    "from gnn_model import StationFlowGCN, StationFlowGAT\n",
    "from train_gnn import(\n",
    "    train_gnn_model,\n",
    "    eval_gnn_model,\n",
    "    regul_edge_node_flow\n",
    ")\n",
    "from utils.station_network import StationNetworkSimul\n",
    "from utils.data import get_degraded_network_loader, create_degraded_networks\n",
    "from utils.plot import(\n",
    "    boxplot_node_metric,\n",
    "    boxplot_node_metric_per_line,\n",
    "    plot_true_predicted,\n",
    "    plot_predicted_ape,\n",
    ")\n",
    "from utils.metrics import (\n",
    "    get_metric_per_node_per_network,\n",
    "    MAPE_loss,\n",
    "    WAPE_loss,\n",
    "    WMAPE_loss, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations = pd.read_csv('data/plan du métro.csv')\n",
    "df_stations = df_stations[~df_stations['vers Ligne'].isin(['\\xa01', '\\xa07', '\\xa02', '\\xa08', '\\xa06'])]\n",
    "\n",
    "df_pos = pd.read_csv(\"data/position gps des stations de métro.csv\")\n",
    "\n",
    "#Removing Malsesherbes RER Station\n",
    "df_pos = df_pos.drop([151])\n",
    "\n",
    "df_flow = pd.read_csv('data/passagers.csv')\n",
    "df_flow['nombre'] = df_flow['nombre'].astype(float)\n",
    "test_network = StationNetworkSimul(df_stations=df_stations, df_pos=df_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNCOMMENT TO GENERATE DATA\n",
    "\n",
    "# test_network.set_edges_weights()\n",
    "# test_network.set_nodes_traffic(test_network.network_graph, df_flow=df_flow)\n",
    "\n",
    "# data_dir = \"graph_dataset/\"\n",
    "# for i in range(1,11):\n",
    "#     create_degraded_networks(test_network, df_flow, num_delete=i, num_degraded=100, data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"graph_dataset/\"\n",
    "\n",
    "train_degraded_graphs = {i : [] for i in range(1,11)}\n",
    "dev_degraded_graphs = {i : [] for i in range(1,11)}\n",
    "test_degraded_graphs = {i : [] for i in range(1,11)}\n",
    "\n",
    "train_test_ratio = 0.9\n",
    "dev_train_ratio = 0.1\n",
    "\n",
    "for i in range(1,3):\n",
    "    folder_path = os.path.join(data_dir, f'delete_{i}')\n",
    "    all_files = [file_path for file_path in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, file_path))]\n",
    "    degraded_graphs = []\n",
    "    for file_path in all_files:\n",
    "        with open(os.path.join(folder_path, file_path), 'rb') as f:\n",
    "            new_net = pickle.load(f)\n",
    "        degraded_graphs.append(new_net)\n",
    "    \n",
    "    train_split_idx = int(train_test_ratio*len(all_files))\n",
    "    dev_split_idx = int(dev_train_ratio*train_split_idx)\n",
    "\n",
    "    dev_degraded_graphs[i].extend(degraded_graphs[:dev_split_idx])\n",
    "    train_degraded_graphs[i].extend(degraded_graphs[dev_split_idx:train_split_idx])\n",
    "    test_degraded_graphs[i].extend(degraded_graphs[train_split_idx:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCNConv Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = dict(\n",
    "    epochs = 200,\n",
    "    lr = 0.001,\n",
    "    criterion = dict(\n",
    "        node=torch.nn.L1Loss(),\n",
    "        edge=torch.nn.L1Loss(),\n",
    "        # regul=regul_edge_node_flow(),\n",
    "    ),\n",
    "    metrics = dict(\n",
    "        MAE=torch.nn.L1Loss(),\n",
    "        MAPE=MAPE_loss()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using node position as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_name = 'traffic'\n",
    "# node_feature_names=['x', 'y']\n",
    "\n",
    "loader_config = dict(\n",
    "    node_target_name = 'traffic',\n",
    "    edge_target_name = 'traffic',\n",
    "    node_feature_names = ['x', 'y'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "total_train_degraded_graphs = list(chain.from_iterable([train_degraded_graphs[i] for i in range(1,11)]))\n",
    "total_dev_degraded_graphs = list(chain.from_iterable([dev_degraded_graphs[i] for i in range(1,11)]))\n",
    "total_test_degraded_graphs = list(chain.from_iterable([test_degraded_graphs[i] for i in range(1,11)]))\n",
    "\n",
    "\n",
    "train_loader = get_degraded_network_loader(total_train_degraded_graphs, **loader_config)\n",
    "dev_loader = get_degraded_network_loader(total_dev_degraded_graphs, **loader_config)\n",
    "test_loader = get_degraded_network_loader(total_test_degraded_graphs, **loader_config)\n",
    "\n",
    "# train_loader = get_degraded_network_loader(train_degraded_graphs, target_name=target_name, node_feature_names=node_feature_names, shuffle=True)\n",
    "# dev_loader = get_degraded_network_loader(dev_degraded_graphs, target_name=target_name, node_feature_names=node_feature_names, shuffle=True)\n",
    "# test_loader = get_degraded_network_loader(test_degraded_graphs, target_name=target_name, node_feature_names=node_feature_names, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "TRAIN: node: 15457.966796875 - edge: 5641.18212890625\n",
      "EVAL: Node - MAE: 10706.5556640625\tMAPE: 1.099364995956421\n",
      "Edge - MAE: 4792.00244140625\tMAPE: inf\n",
      "\n",
      "Epoch 1\n",
      "TRAIN: node: 13075.29296875 - edge: 5215.0625\n",
      "EVAL: Node - MAE: 10704.0400390625\tMAPE: 1.1239451169967651\n",
      "Edge - MAE: 4792.32666015625\tMAPE: inf\n",
      "\n",
      "Epoch 2\n",
      "TRAIN: node: 12281.10546875 - edge: 5072.49072265625\n",
      "EVAL: Node - MAE: 10705.5087890625\tMAPE: 1.135325312614441\n",
      "Edge - MAE: 4787.94677734375\tMAPE: inf\n",
      "\n",
      "Epoch 3\n",
      "TRAIN: node: 11884.0048828125 - edge: 4998.91943359375\n",
      "EVAL: Node - MAE: 10704.58203125\tMAPE: 1.1117364168167114\n",
      "Edge - MAE: 4770.63916015625\tMAPE: inf\n",
      "\n",
      "Epoch 4\n",
      "TRAIN: node: 11645.7236328125 - edge: 4950.0419921875\n",
      "EVAL: Node - MAE: 10704.4345703125\tMAPE: 1.1127910614013672\n",
      "Edge - MAE: 4742.71630859375\tMAPE: inf\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 10\u001b[0m\n\u001b[0;32m      2\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      4\u001b[0m nodes_gnn_model \u001b[38;5;241m=\u001b[39m StationFlowGCN(\n\u001b[0;32m      5\u001b[0m     input_dim\u001b[38;5;241m=\u001b[39minput_dim,\n\u001b[0;32m      6\u001b[0m     output_dim\u001b[38;5;241m=\u001b[39moutput_dim,\n\u001b[0;32m      7\u001b[0m     num_nodes\u001b[38;5;241m=\u001b[39mtrain_loader\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m      8\u001b[0m )\n\u001b[1;32m---> 10\u001b[0m \u001b[43mtrain_gnn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes_gnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tengo\\Documents\\Cours IAM - IAAA\\IAM\\Projet 3A\\FlowForecasting\\ratp\\train_gnn.py:49\u001b[0m, in \u001b[0;36mtrain_gnn_model\u001b[1;34m(node_gnn_model, config, train_loader, dev_loader)\u001b[0m\n\u001b[0;32m     46\u001b[0m         loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\u001b[38;5;241m*\u001b[39mregul_loss\n\u001b[0;32m     47\u001b[0m         epoch_loss[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregul\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(regul_loss\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[1;32m---> 49\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m type_loss \u001b[38;5;129;01min\u001b[39;00m train_loss\u001b[38;5;241m.\u001b[39mkeys():\n",
      "File \u001b[1;32mc:\\Users\\tengo\\Documents\\Cours IAM - IAAA\\IAM\\Projet 3A\\FlowForecasting\\env\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tengo\\Documents\\Cours IAM - IAAA\\IAM\\Projet 3A\\FlowForecasting\\env\\lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tengo\\Documents\\Cours IAM - IAAA\\IAM\\Projet 3A\\FlowForecasting\\env\\lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_dim = train_loader.dataset[0].x.shape[1]\n",
    "output_dim = 1\n",
    "\n",
    "nodes_gnn_model = StationFlowGCN(\n",
    "    input_dim=input_dim,\n",
    "    output_dim=output_dim,\n",
    "    num_nodes=train_loader.dataset[0].x.shape[0],\n",
    ")\n",
    "\n",
    "train_gnn_model(nodes_gnn_model, train_config, train_loader, dev_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = eval_gnn_model(nodes_gnn_model, test_loader,train_config)\n",
    "print(\"\\t\".join([f\"{metric_name}: {metric_value}\" for metric_name, metric_value in test_metrics.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = random.choice(test_loader.dataset)\n",
    "x, edge_index = test_data.x, test_data.edge_index\n",
    "output = nodes_gnn_model(x, edge_index)\n",
    "print(output.shape)\n",
    "actual_flows = test_data.y\n",
    "predicted_flows = output.detach().squeeze(1)\n",
    "\n",
    "plot_true_predicted(predicted_flows, actual_flows)\n",
    "plot_predicted_ape(predicted_flows, actual_flows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = random.choice(test_loader.dataset)\n",
    "x, edge_index = test_data.x, test_data.edge_index\n",
    "output, _ = nodes_gnn_model(x, edge_index)\n",
    "\n",
    "actual_flows = test_data.y\n",
    "predicted_flows = output.detach().squeeze(1)\n",
    "\n",
    "plot_true_predicted(predicted_flows, actual_flows)\n",
    "plot_predicted_ape(predicted_flows, actual_flows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using node embedding as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialized + updated during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Unweighted edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_config = dict(\n",
    "    node_target_name = 'traffic',\n",
    "    edge_target_name = 'traffic',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "total_train_degraded_graphs = list(chain.from_iterable([train_degraded_graphs[i] for i in range(1,11)]))\n",
    "total_dev_degraded_graphs = list(chain.from_iterable([dev_degraded_graphs[i] for i in range(1,11)]))\n",
    "total_test_degraded_graphs = list(chain.from_iterable([test_degraded_graphs[i] for i in range(1,11)]))\n",
    "\n",
    "train_loader = get_degraded_network_loader(total_train_degraded_graphs, **loader_config)\n",
    "dev_loader = get_degraded_network_loader(total_dev_degraded_graphs, **loader_config)\n",
    "test_loader = get_degraded_network_loader(total_test_degraded_graphs, **loader_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "TRAIN: node: 14464.279296875 - edge: 5207.67724609375\n",
      "EVAL: Node - MAE: 4867.54833984375\tMAPE: 0.33442020416259766\n",
      "Edge - MAE: 3460.5595703125\tMAPE: inf\n",
      "\n",
      "Epoch 1\n",
      "TRAIN: node: 9413.271484375 - edge: 4303.66943359375\n",
      "EVAL: Node - MAE: 4072.57763671875\tMAPE: 0.28762316703796387\n",
      "Edge - MAE: 3351.351318359375\tMAPE: inf\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 11\u001b[0m\n\u001b[0;32m      2\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      4\u001b[0m nodes_gnn_model \u001b[38;5;241m=\u001b[39m StationFlowGCN(\n\u001b[0;32m      5\u001b[0m     input_dim\u001b[38;5;241m=\u001b[39minput_dim,\n\u001b[0;32m      6\u001b[0m     output_dim\u001b[38;5;241m=\u001b[39moutput_dim,\n\u001b[0;32m      7\u001b[0m     num_nodes\u001b[38;5;241m=\u001b[39mtrain_loader\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m      8\u001b[0m     embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitialized\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m )\n\u001b[1;32m---> 11\u001b[0m \u001b[43mtrain_gnn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes_gnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tengo\\Documents\\Cours IAM - IAAA\\IAM\\Projet 3A\\FlowForecasting\\ratp\\train_gnn.py:33\u001b[0m, in \u001b[0;36mtrain_gnn_model\u001b[1;34m(node_gnn_model, config, train_loader, dev_loader)\u001b[0m\n\u001b[0;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     32\u001b[0m x, edge_index, edge_weight \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index, data\u001b[38;5;241m.\u001b[39medge_attr\n\u001b[1;32m---> 33\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mnode_gnn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node_criterion \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\tengo\\Documents\\Cours IAM - IAAA\\IAM\\Projet 3A\\FlowForecasting\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tengo\\Documents\\Cours IAM - IAAA\\IAM\\Projet 3A\\FlowForecasting\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tengo\\Documents\\Cours IAM - IAAA\\IAM\\Projet 3A\\FlowForecasting\\ratp\\gnn_model.py:58\u001b[0m, in \u001b[0;36mStationFlowGCN.forward\u001b[1;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[0;32m     55\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_emb(x)\n\u001b[0;32m     57\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x, edge_index, edge_weight)\n\u001b[1;32m---> 58\u001b[0m x_e \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m node_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_head(x)\n\u001b[0;32m     61\u001b[0m edge_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_head(x_e)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_dim = 100\n",
    "output_dim = 1\n",
    "\n",
    "nodes_gnn_model = StationFlowGCN(\n",
    "    input_dim=input_dim,\n",
    "    output_dim=output_dim,\n",
    "    num_nodes=train_loader.dataset[0].x.shape[0],\n",
    "    embeddings='initialized'\n",
    ")\n",
    "\n",
    "train_gnn_model(nodes_gnn_model, train_config, train_loader, dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = eval_gnn_model(nodes_gnn_model, test_loader,train_config)\n",
    "print(\"\\t\".join([f\"{metric_name}: {metric_value}\" for metric_name, metric_value in test_metrics.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = np.random.randint(0, len(test_loader))\n",
    "test_data = test_loader.dataset[random_idx]\n",
    "\n",
    "x, edge_index = test_data.x, test_data.edge_index\n",
    "node_output, edge_output  = nodes_gnn_model(x, edge_index)\n",
    "\n",
    "node_actual_flows = test_data.y\n",
    "edge_actual_flows = test_data.ye\n",
    "\n",
    "node_predicted_flows= node_output.detach().squeeze(1)\n",
    "edge_predicted_flows= edge_output.detach().squeeze(1)\n",
    "\n",
    "current_edges = list(zip(edge_index[0].numpy(), edge_index[1].numpy()))\n",
    "removed_edges = list(set(test_network.network_graph.edges) - set(current_edges))\n",
    "\n",
    "removed_edges_name = [\n",
    "    f'{test_network.reverse_network_stations[edge[0]][\"title\"]} ({test_network.reverse_network_stations[edge[0]][\"group\"]}) -> {test_network.reverse_network_stations[edge[1]][\"title\"] } ({test_network.reverse_network_stations[edge[1]][\"group\"]})'\n",
    "    for edge in removed_edges\n",
    "    ]\n",
    "\n",
    "plot_true_predicted(node_predicted_flows, node_actual_flows, removed_edges_name=removed_edges_name)\n",
    "plot_true_predicted(edge_predicted_flows, edge_actual_flows, removed_edges_name=removed_edges_name)\n",
    "plot_predicted_ape(node_predicted_flows, node_actual_flows, removed_edges_name=removed_edges_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using edges weights (based on node position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_config = dict(\n",
    "    node_target_name = 'traffic',\n",
    "    edge_target_name = 'traffic',\n",
    "    edge_feature_names = ['weight'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "total_train_degraded_graphs = list(chain.from_iterable([train_degraded_graphs[i] for i in range(1,11)]))\n",
    "total_dev_degraded_graphs = list(chain.from_iterable([dev_degraded_graphs[i] for i in range(1,11)]))\n",
    "total_test_degraded_graphs = list(chain.from_iterable([test_degraded_graphs[i] for i in range(1,11)]))\n",
    "\n",
    "\n",
    "train_loader = get_degraded_network_loader(total_train_degraded_graphs, **loader_config)\n",
    "dev_loader = get_degraded_network_loader(total_dev_degraded_graphs, **loader_config)\n",
    "test_loader = get_degraded_network_loader(total_test_degraded_graphs, **loader_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 100\n",
    "output_dim = 1\n",
    "\n",
    "nodes_gnn_model = StationFlowGCN(\n",
    "    input_dim=input_dim,\n",
    "    output_dim=output_dim,\n",
    "    num_nodes=train_loader.dataset[0].x.shape[0],\n",
    "    embeddings='initialized'\n",
    ")\n",
    "\n",
    "train_gnn_model(nodes_gnn_model, train_config, train_loader, dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = eval_gnn_model(nodes_gnn_model, test_loader, train_config)\n",
    "print(\"\\t\".join([f\"{metric_name}: {metric_value}\" for metric_name, metric_value in test_metrics.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = MAPE_loss(reduction='none')\n",
    "df_ape_per_node_per_network = get_metric_per_node_per_network(nodes_gnn_model, test_loader, metric, test_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_node_metric(df_ape_per_node_per_network, node_idx=12, network_simul=test_network, metric_name='APE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_node_metric_per_line(df_ape_per_node_per_network, '7', test_network, 'APE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_loader.dataset[18]\n",
    "x, edge_index = test_data.x, test_data.edge_index\n",
    "node_output, edge_output  = nodes_gnn_model(x, edge_index)\n",
    "\n",
    "node_actual_flows = test_data.y\n",
    "edge_actual_flows = test_data.ye\n",
    "\n",
    "node_predicted_flows= node_output.detach().squeeze(1)\n",
    "edge_predicted_flows= edge_output.detach().squeeze(1)\n",
    "\n",
    "current_edges = list(zip(edge_index[0].numpy(), edge_index[1].numpy()))\n",
    "removed_edges = list(set(test_network.network_graph.edges) - set(current_edges))\n",
    "\n",
    "removed_edges_name = [\n",
    "    f'{test_network.reverse_network_stations[edge[0]][\"title\"]} ({test_network.reverse_network_stations[edge[0]][\"group\"]}) -> {test_network.reverse_network_stations[edge[1]][\"title\"] } ({test_network.reverse_network_stations[edge[1]][\"group\"]})'\n",
    "    for edge in removed_edges\n",
    "    ]\n",
    "\n",
    "plot_true_predicted(node_predicted_flows, node_actual_flows, removed_edges_name=removed_edges_name)\n",
    "plot_predicted_ape(node_predicted_flows, actual_flows)\n",
    "plot_true_predicted(edge_predicted_flows, edge_actual_flows, removed_edges_name=removed_edges_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Node2Vec embeddings as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_idx = torch.tensor([i for i in sorted(test_network.network_graph.nodes)], dtype=torch.int)\n",
    "tensor_edges = torch.tensor([\n",
    "    [edge[0] for edge in sorted(test_network.network_graph.edges)],\n",
    "    [edge[1] for edge in sorted(test_network.network_graph.edges)]\n",
    "    ], dtype=torch.long)\n",
    "init_data_graph = Data(x=node_idx, edge_index=tensor_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_n2v = Node2Vec(\n",
    "    init_data_graph.edge_index,\n",
    "    embedding_dim=100,\n",
    "    walks_per_node=10,\n",
    "    walk_length=20,\n",
    "    context_size=10,\n",
    "    p=1.0,\n",
    "    q=1.0,\n",
    "    num_negative_samples=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_n2v = model_n2v.loader(batch_size=128, shuffle=True)\n",
    "optimizer_n2v = optim.Adam(model_n2v.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_n2v.train()\n",
    "for pos_rw, neg_rw in loader_n2v:\n",
    "    optimizer_n2v.zero_grad()\n",
    "    loss = model_n2v.loss(pos_rw, neg_rw)\n",
    "    loss.backward()\n",
    "    optimizer_n2v.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_n2v = model_n2v()\n",
    "isinstance(embeddings_n2v, torch.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_config = dict(\n",
    "    node_target_name = 'traffic',\n",
    "    edge_target_name = 'traffic',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "total_train_degraded_graphs = list(chain.from_iterable([train_degraded_graphs[i] for i in range(1,11)]))\n",
    "total_dev_degraded_graphs = list(chain.from_iterable([dev_degraded_graphs[i] for i in range(1,11)]))\n",
    "total_test_degraded_graphs = list(chain.from_iterable([test_degraded_graphs[i] for i in range(1,11)]))\n",
    "\n",
    "\n",
    "train_loader = get_degraded_network_loader(total_train_degraded_graphs, **loader_config)\n",
    "dev_loader = get_degraded_network_loader(total_dev_degraded_graphs, **loader_config)\n",
    "test_loader = get_degraded_network_loader(total_test_degraded_graphs, **loader_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 100\n",
    "output_dim = 1\n",
    "\n",
    "nodes_gnn_model = StationFlowGCN(\n",
    "    input_dim=input_dim,\n",
    "    output_dim=output_dim,\n",
    "    num_nodes=train_loader.dataset[0].x.shape[0],\n",
    "    embeddings=embeddings_n2v,\n",
    "    freeze=True\n",
    ")\n",
    "\n",
    "train_gnn_model(nodes_gnn_model, train_config, train_loader, dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = eval_gnn_model(nodes_gnn_model, test_loader,train_config)\n",
    "print(\"\\t\".join([f\"{metric_name}: {metric_value}\" for metric_name, metric_value in test_metrics.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GATConv Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = dict(\n",
    "    epochs = 20,\n",
    "    lr = 0.01,\n",
    "    criterion = dict(\n",
    "        node=torch.nn.L1Loss(),\n",
    "        edge=torch.nn.L1Loss(),\n",
    "        # regul=regul_edge_node_flow(),\n",
    "    ),\n",
    "    metrics = dict(\n",
    "        MAE=torch.nn.L1Loss(),\n",
    "        MAPE=MAPE_loss()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using node embedding as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialized + updated during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Unweighted edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_config = dict(\n",
    "    node_target_name = 'traffic',\n",
    "    edge_target_name = 'traffic',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "total_train_degraded_graphs = list(chain.from_iterable([train_degraded_graphs[i] for i in range(1,11)]))\n",
    "total_dev_degraded_graphs = list(chain.from_iterable([dev_degraded_graphs[i] for i in range(1,11)]))\n",
    "total_test_degraded_graphs = list(chain.from_iterable([test_degraded_graphs[i] for i in range(1,11)]))\n",
    "\n",
    "\n",
    "train_loader = get_degraded_network_loader(total_train_degraded_graphs, **loader_config)\n",
    "dev_loader = get_degraded_network_loader(total_dev_degraded_graphs, **loader_config)\n",
    "test_loader = get_degraded_network_loader(total_test_degraded_graphs, **loader_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 100\n",
    "output_dim = 1\n",
    "\n",
    "nodes_gnn_model = StationFlowGAT(\n",
    "    input_dim=input_dim,\n",
    "    output_dim=output_dim,\n",
    "    num_nodes=train_loader.dataset[0].x.shape[0],\n",
    "    embeddings='initialized',\n",
    "    num_heads=4\n",
    ")\n",
    "\n",
    "train_gnn_model(nodes_gnn_model, train_config, train_loader, dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = eval_gnn_model(nodes_gnn_model, test_loader,train_config)\n",
    "print(\"\\t\".join([f\"{metric_name}: {metric_value}\" for metric_name, metric_value in test_metrics.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Weighted edges (based on node position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_config = dict(\n",
    "    node_target_name = 'traffic',\n",
    "    edge_target_name = 'traffic',\n",
    "    edge_feature_names = ['weight'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "total_train_degraded_graphs = list(chain.from_iterable([train_degraded_graphs[i] for i in range(1,11)]))\n",
    "total_dev_degraded_graphs = list(chain.from_iterable([dev_degraded_graphs[i] for i in range(1,11)]))\n",
    "total_test_degraded_graphs = list(chain.from_iterable([test_degraded_graphs[i] for i in range(1,11)]))\n",
    "\n",
    "\n",
    "train_loader = get_degraded_network_loader(total_train_degraded_graphs, **loader_config)\n",
    "dev_loader = get_degraded_network_loader(total_dev_degraded_graphs, **loader_config)\n",
    "test_loader = get_degraded_network_loader(total_test_degraded_graphs, **loader_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 100\n",
    "output_dim = 1\n",
    "\n",
    "nodes_gnn_model = StationFlowGAT(\n",
    "    input_dim=input_dim,\n",
    "    output_dim=output_dim,\n",
    "    num_nodes=train_loader.dataset[0].x.shape[0],\n",
    "    embeddings='initialized',\n",
    "    edge_dim=1\n",
    ")\n",
    "\n",
    "train_gnn_model(nodes_gnn_model, train_config, train_loader, dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = eval_gnn_model(nodes_gnn_model, test_loader,train_config)\n",
    "print(\"\\t\".join([f\"{metric_name}: {metric_value}\" for metric_name, metric_value in test_metrics.items()]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
