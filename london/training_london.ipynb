{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split, Dataset\n",
    " \n",
    "from tqdm import tqdm\n",
    "from utils.process_data import build_train_test_loaders, build_train_test_loaders_2\n",
    "from utils.training import CustomMAELoss, CustomMAPELoss, test_model, get_flow_forecasting_metrics\n",
    "from utils.link_loads import get_graph_attributes, df_to_graph, build_quarter_hour_data, add_missing_nodes, add_missing_nodes_2\n",
    "from utils.models import STGCN\n",
    "# import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Charger les csv\n",
    "- Supprimer les horaires 00h - 05h\n",
    "- Les save by jour\n",
    "- Créer la class Dataset qui renvoie le graph actuel avec les flows précédent dans la journée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"data/\"\n",
    "# Get graph attributes, create dfs from csv, process Link column and ordered dfs by time\n",
    "num_nodes, edge_index, node_mapping, dfs = get_graph_attributes(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE = CustomMAELoss()\n",
    "MAPE = CustomMAPELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN + GRU (pas concluant, ne pas run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data = []\n",
    "# each df should have the same dimension and same nodes at the same columns\n",
    "for filename, df in dfs.items():\n",
    "    df = add_missing_nodes(df, node_mapping, num_nodes) # add zeros row for missing nodes\n",
    "    df_qhrs = build_quarter_hour_data(df, filename, num_nodes) # retourne 24*4 df avec ses paramètres temporel et le flow\n",
    "    graph_data.extend(df_qhrs)\n",
    "    \n",
    "graphs = [df_to_graph(df, edge_index) for df in graph_data]  # Un graphe par quart d'heure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TGCN(nn.Module):\n",
    "    def __init__(self, node_features, hidden_dim, gru_hidden_dim):\n",
    "        super(TGCN, self).__init__()\n",
    "        self.gcn = GCNConv(node_features, hidden_dim)  # GCN\n",
    "        self.gru = nn.GRU(hidden_dim, gru_hidden_dim, batch_first=True)  # GRU\n",
    "        self.fc = nn.Linear(gru_hidden_dim, 1)  # Prédiction finale\n",
    "\n",
    "    def forward(self, graph_seq):\n",
    "        # window_size = len(graph_seq)  # Nombre de pas de temps\n",
    "        # batch_size = graph_seq[0].x.shape[0]  # Nombre de nœuds\n",
    "\n",
    "        spatial_features = []\n",
    "        for graph in graph_seq:\n",
    "            x = self.gcn(graph.x, graph.edge_index)  # GCN\n",
    "            x = F.relu(x)\n",
    "            spatial_features.append(x)\n",
    "\n",
    "        spatial_features = torch.stack(spatial_features, dim=1)  # (batch, time, hidden_dim)\n",
    "\n",
    "        _ , final_state = self.gru(spatial_features)  # prédiction sur le dernier (1, gru_hidden_dim)\n",
    "        final_state = final_state.squeeze() # (gru_hidden_dim)\n",
    "        final_out = self.fc(final_state) # Prédiction sur le dernier état\n",
    "        final_out = F.relu(final_out)\n",
    "        return final_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TGCN(node_features=10, hidden_dim=32, gru_hidden_dim=64)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = MAE\n",
    "\n",
    "# Boucle d'entraînement\n",
    "for epoch in tqdm(range(50)):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for graph_seq, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(graph_seq)\n",
    "\n",
    "        target = target.squeeze()\n",
    "        output = output.reshape(target.shape)\n",
    "    \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for graph_seq, target in test_loader:\n",
    "\n",
    "        output = model(graph_seq)\n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "print(f\"Test MAE: {test_loss / len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "39e6/(96*1206)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average flow per inter_station per 15min is 337. So 204 MAE error is not a valid result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN with past flows as node features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data = []\n",
    "# each df should have the same dimension and same nodes at the same columns\n",
    "for filename, df in dfs.items():\n",
    "    df = add_missing_nodes(df, node_mapping, num_nodes) # add zeros row for missing nodes\n",
    "    df_qhrs = build_quarter_hour_data(df, filename, num_nodes) # retourne 24*4 df avec ses paramètres temporel et le flow\n",
    "    graph_data.extend(df_qhrs)\n",
    "    \n",
    "graphs = [df_to_graph(df, edge_index) for df in graph_data]  # Un graphe par quart d'heure\n",
    "\n",
    "window_size=4\n",
    "\n",
    "train_loader, test_loader = build_train_test_loaders(graphs, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En utilisant la dernière valeur du flow comme prédiction :\n",
      "Test MAE: 21.719919204711914\n"
     ]
    }
   ],
   "source": [
    "loss_mae = 0\n",
    "for graph, temporal_features in test_loader:\n",
    "    output = graph.x[:,-1]\n",
    "    output = output.unsqueeze(-1)\n",
    "    target = graph.y\n",
    "\n",
    "    loss_mae += MAE(output, target)\n",
    "\n",
    "print(\"En utilisant la dernière valeur du flow comme prédiction :\")\n",
    "print(f\"Test MAE: {loss_mae / len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_nan_or_empty(tensor):\n",
    "    return tensor.numel() == 0 or torch.isnan(tensor).all()\n",
    "\n",
    "loss_mae = 0\n",
    "loss_mape = 0\n",
    "for graph, temporal_features in test_loader:\n",
    "    output = graph.x[:,-1]\n",
    "    output = output.unsqueeze(-1)\n",
    "    target = graph.y\n",
    "\n",
    "    maskP = target >= 1\n",
    "    countP = maskP.sum().item()\n",
    "\n",
    "    outputP = output[maskP]\n",
    "    targetP = target[maskP]\n",
    "\n",
    "    if not (is_nan_or_empty(outputP) or is_nan_or_empty(targetP)):\n",
    "        loss_mae += MAE(outputP, targetP)\n",
    "        loss_mape += MAPE(outputP, targetP)\n",
    "\n",
    "print(\"En utilisant la dernière valeur du flow comme prédiction :\")\n",
    "print(f\"Test MAE: {loss_mae / len(test_loader)}\")\n",
    "print(f\"Test MAPE: {loss_mape / len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAPE pas de sens ici à cause des valeurs nulles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compliqué d'utiliser la MAPE car trop de targets égale ou proche de zéro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN 0500-0000 flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Data loader\n",
    "graph_data = []\n",
    "# each df should have the same dimension and same nodes at the same columns\n",
    "for filename, df in dfs.items():\n",
    "    df = add_missing_nodes_2(df, node_mapping, num_nodes) # add zeros row for missing nodes\n",
    "    df_qhrs = build_quarter_hour_data(df, filename, num_nodes) # retourne 24*4 df avec ses paramètres temporel et le flow\n",
    "    graph_data.append(df_qhrs)\n",
    "graphs = [[df_to_graph(df, edge_index) for df in graph_data[i]] for i in range(len(graph_data))]  # Un graphe par quart d'heure\n",
    "\n",
    "train_loader, test_loader = build_train_test_loaders_2(graphs, window_size, train_split=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En utilisant la dernière valeur du flow comme prédiction :\n",
      "Test MAE: 25.606801986694336\n",
      "MAPE for high targets: 0.07953206537705329\n",
      "MAE for low targets: 8.892926945572807\n"
     ]
    }
   ],
   "source": [
    "# Performance de la Baseline (prédiction du dernier flow)\n",
    "loss_mae = 0\n",
    "loss_mape = 0\n",
    "count_total = 0\n",
    "MAPE_high_target = 0\n",
    "MAE_low_target = 0\n",
    "\n",
    "for graph, temporal_features in test_loader:\n",
    "    output = graph.x[:,-1]\n",
    "    output = output.unsqueeze(-1)\n",
    "    target = graph.y\n",
    "    loss_mae += MAE(output, target)\n",
    "    loss_mape += MAPE(output, target)\n",
    "    MAPE_high_target_ , MAE_low_target_ = get_flow_forecasting_metrics(output, target)\n",
    "    MAPE_high_target += float(MAPE_high_target_)\n",
    "    MAE_low_target += float(MAE_low_target_)\n",
    "\n",
    "print(\"En utilisant la dernière valeur du flow comme prédiction :\")\n",
    "print(f\"Test MAE: {loss_mae / len(test_loader)}\")\n",
    "\n",
    "print(f\"MAPE for high targets: {MAPE_high_target / len(test_loader)}\")\n",
    "print(f\"MAE for low targets: {MAE_low_target/ len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif étant de faire mieux que cette base\n",
    "\n",
    "Ci dessous voici les performances de la baseline sans prendre en compte les valeurs nulles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_nan_or_empty(tensor):\n",
    "    return tensor.numel() == 0 or torch.isnan(tensor).all()\n",
    "\n",
    "loss_mae = 0\n",
    "loss_mape = 0\n",
    "for graph, temporal_features in test_loader:\n",
    "    output = graph.x[:,-1]\n",
    "    output = output.unsqueeze(-1)\n",
    "    target = graph.y\n",
    "\n",
    "    maskP = target >= 1\n",
    "    countP = maskP.sum().item()\n",
    "\n",
    "    outputP = output[maskP]\n",
    "    targetP = target[maskP]\n",
    "\n",
    "    if not (is_nan_or_empty(outputP) or is_nan_or_empty(targetP)):\n",
    "        loss_mae += MAE(outputP, targetP)\n",
    "        loss_mape += MAPE(outputP, targetP)\n",
    "\n",
    "print(\"En utilisant la dernière valeur du flow comme prédiction :\")\n",
    "print(f\"Test MAE: {loss_mae / len(test_loader)}\")\n",
    "print(f\"Test MAPE: {loss_mape / len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train loss: 33.027031764586766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 1/40 [00:13<08:33, 13.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Test loss: 23.617153944477202\n",
      "Epoch 2, Train loss: 19.945948464738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2/40 [00:25<08:04, 12.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Test loss: 24.19250312967906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2/40 [00:26<08:27, 13.36s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m target \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39my    \n\u001b[0;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[1;32m---> 19\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()        \n\u001b[0;32m     21\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\tengo\\Documents\\Cours IAM - IAAA\\IAM\\Projet 3A\\FlowForecasting\\env\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tengo\\Documents\\Cours IAM - IAAA\\IAM\\Projet 3A\\FlowForecasting\\env\\lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tengo\\Documents\\Cours IAM - IAAA\\IAM\\Projet 3A\\FlowForecasting\\env\\lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = STGCN(node_features=window_size, hidden_dim=32)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "criterion = MAE\n",
    "\n",
    "num_epochs = 40\n",
    "\n",
    "# Boucle d'entraînement\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for graph, temporal_features in train_loader:\n",
    "        temporal_features = temporal_features.squeeze()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(graph, temporal_features)\n",
    "        target = graph.y    \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Train loss: {train_loss / len(train_loader)}\")\n",
    "\n",
    "    # Évaluation sur le jeu de test\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():  # Désactivation des gradients pour l'évaluation\n",
    "        for graph, temporal_features in test_loader:\n",
    "            temporal_features = temporal_features.squeeze()  # Squeeze si nécessaire\n",
    "            output = model(graph, temporal_features)  # Assure-toi de passer aussi les temporal_features\n",
    "            target = graph.y\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Test loss: {test_loss / len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve the last model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.models import ImprovedSTGCN\n",
    "model = ImprovedSTGCN(node_features=window_size, hidden_dim=16)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "criterion = MAE\n",
    "\n",
    "num_epochs = 40\n",
    "\n",
    "# Boucle d'entraînement\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for graph, temporal_features in train_loader:\n",
    "        temporal_features = temporal_features.squeeze()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(graph, temporal_features)\n",
    "        target = graph.y    \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Train loss: {train_loss / len(train_loader)}\")\n",
    "\n",
    "    # Évaluation sur le jeu de test\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():  # Désactivation des gradients pour l'évaluation\n",
    "        for graph, temporal_features in test_loader:\n",
    "            temporal_features = temporal_features.squeeze()  # Squeeze si nécessaire\n",
    "            output = model(graph, temporal_features)  # Assure-toi de passer aussi les temporal_features\n",
    "            target = graph.y\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Test loss: {test_loss / len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 0\n",
    "for graph, temporal_features in test_loader:\n",
    "    y = graph.y\n",
    "    mean += y.mean()\n",
    "print(f\"Average of passengers per inter-stations (nodes): {mean/len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Définition des quantiles à calculer\n",
    "quantiles = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "quantile_values = {q: 0 for q in quantiles}\n",
    "\n",
    "# Calcul des quantiles sur l'ensemble du test_loader\n",
    "for graph, temporal_features in test_loader:\n",
    "    y = graph.y\n",
    "    for q in quantiles:\n",
    "        quantile_values[q] += y.quantile(q)\n",
    "\n",
    "# Moyenne des quantiles sur l'ensemble du dataset\n",
    "num_graphs = len(test_loader)\n",
    "quantile_averages = {q: quantile_values[q] / num_graphs for q in quantiles}\n",
    "\n",
    "# Tracé du graphique\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(quantiles, list(quantile_averages.values()), marker='o', linestyle='-', color='b')\n",
    "plt.xlabel(\"Quantiles\")\n",
    "plt.ylabel(\"Valeur moyenne du flux\")\n",
    "plt.title(\"Quantiles moyens des flux de passagers par inter-station\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_averages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
